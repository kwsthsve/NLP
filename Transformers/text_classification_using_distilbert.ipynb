{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJlujT5FY2NV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487b3ce3-b45a-418d-f9e2-32c490e9faa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import classification_report, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNm2dKl1peT9"
      },
      "source": [
        "# Load IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FNCkDzwTB4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8d365f-7557-42a8-ded1-4c779c6a77ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set shape : (30000,)\n",
            "Development set shape : (10000,)\n",
            "Test set shape : (10000,)\n",
            "\n",
            "Average doc length of training set : 238\n",
            "Average doc length of development set : 242\n",
            "Average doc length of test set : 234\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
        "    path='imdb.npz',\n",
        "    num_words=None,\n",
        "    skip_top=0,\n",
        "    maxlen=None,\n",
        "    seed=113,\n",
        "    start_char=1,\n",
        "    oov_char=2,\n",
        "    index_from=3)\n",
        "\n",
        "\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "start_char = 1\n",
        "oov_char = 2\n",
        "\n",
        "\n",
        "inverted_word_index = dict((i + 3, word) for (word, i) in word_index.items())\n",
        "\n",
        "inverted_word_index[start_char] = '[START]'\n",
        "inverted_word_index[oov_char] = '[OOV]'\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "  x_train[i] = ' '.join(inverted_word_index[i] for i in x_train[i])\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  x_test[i] = ' '.join(inverted_word_index[i] for i in x_test[i])\n",
        "\n",
        "\n",
        "\n",
        "x_train = np.concatenate((x_train, x_test[10000:]))\n",
        "y_train = np.concatenate((y_train, y_test[10000:]))\n",
        "\n",
        "x_dev = x_train[:10000]\n",
        "y_dev = y_train[:10000]\n",
        "\n",
        "x_train = x_train[10000:]\n",
        "y_train = y_train[10000:]\n",
        "\n",
        "x_test = x_test[:10000]\n",
        "y_test = y_test[:10000]\n",
        "\n",
        "print('\\nTraining set shape :', x_train.shape)\n",
        "print('Development set shape :', x_dev.shape)\n",
        "print('Test set shape :', x_test.shape)\n",
        "\n",
        "\n",
        "# flatten_x_train = [token for doc in x_train for token in nltk.tokenize.word_tokenize(doc)]\n",
        "# print('\\nVocabulary size :', len(set(flatten_x_train)) - 3)\n",
        "\n",
        "\n",
        "def average_doc_length(docs_tokenized):\n",
        "  doc_sizes = []\n",
        "\n",
        "  for doc in docs_tokenized:\n",
        "    doc_sizes.append(len(doc))\n",
        "\n",
        "  return int(np.mean(np.array(doc_sizes)))\n",
        "\n",
        "\n",
        "x_train_tokenized = [nltk.tokenize.word_tokenize(doc)[3:] for doc in x_train]\n",
        "x_dev_tokenized = [nltk.tokenize.word_tokenize(doc)[3:] for doc in x_dev]\n",
        "x_test_tokenized = [nltk.tokenize.word_tokenize(doc)[3:] for doc in x_test]\n",
        "\n",
        "print('\\nAverage doc length of training set :', average_doc_length(x_train_tokenized))\n",
        "print('Average doc length of development set :', average_doc_length(x_dev_tokenized))\n",
        "print('Average doc length of test set :', average_doc_length(x_test_tokenized))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc7oY76GhIdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2919c088-de94-4981-a020-72bf434775c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"i think this is one of the weakest of the kenneth branagh shakespearian works after such great efforts as much ado about nothing etc i thought this was poor the cast was weaker alicia silverstone nivoli mcelhone but my biggest gripe was that they messed with the bard 's work and cut out some of the play to put in the musical dance sequences br br you just do n't do shakespeare and then mess with the play sorry but that is just wrong i love some cole porter just like the next person but jeez do n't mess with the shakespeare skip this and watch prospero 's books if you want to see a brilliant shakespearean adaptation of the tempest\", 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "target_names = ['negative', 'positive']\n",
        "\n",
        "\n",
        "# We use only some of the reviews that we have to fine-tune the model\n",
        "\n",
        "train_set = [' '.join(x_doc) for x_doc in x_train_tokenized[:1000]]\n",
        "dev_set = [' '.join(x_doc) for x_doc in x_dev_tokenized[:100]]\n",
        "\n",
        "def docs_to_dict(x, y):\n",
        "\n",
        "  dataset = list()\n",
        "\n",
        "  for i in range(len(x)):\n",
        "    data = dict()\n",
        "    data['text'] = x[i]\n",
        "    data['label'] = y[i]\n",
        "\n",
        "    dataset.append(data)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "train_dataset = docs_to_dict(train_set, y_train)\n",
        "dev_dataset = docs_to_dict(dev_set, y_dev)\n",
        "\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] sentence-transformers datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "rvOVPvaHJZgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad24e6c-fb64-416a-f6f6-1c6aed80b289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.25.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agf4yKl4uWwy"
      },
      "source": [
        "# Tokenize to make the format for Distilled BERT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a15jNN4LuXmF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "80dfd1b9996a43658139c86e6aeeb83b",
            "0953473037b741479fbc85ca00942ef8",
            "d859419f0b2d4d0ca004473aeeac8694",
            "153c329214c54c85be6470c8bfa2b410",
            "034bbc8e52c54206988ac58209492cde",
            "26a12e889ab944419e85f2e967337061",
            "0f1186ee76ef4191b5b0d98febaedfb5",
            "790a5ce5bb3945699673ce2df5ce1c34",
            "7633a074e1a34615b84e2ed749b5cc55",
            "7810bc4ddb0d425ea75ab36f4b968e98",
            "c4cd86cb33e843b08ec552c864a9cd8e",
            "f78553923d384895a5fdd210153ee1bc",
            "a2552bfc23db49cea84d421d85ff357c",
            "004ec604b7f943ee9c89f5e4f4aafb78",
            "50f40a9bc5df4d638c5f82a2bfef11ed",
            "84376e23d7934bab8f904e5cd1656751",
            "d4bf80931cf84221ba79a78a5613d914",
            "0edbe93569784b42af374bb6f56e4ea0",
            "ac38470dd42148a086e7435fd12fdd8b",
            "8a03cf31fb0c4334b63146398c37031b",
            "cbb13cab0cef45c3874484a0f3e37a35",
            "c7ca243a068643bdb6aefe386751f771"
          ]
        },
        "outputId": "a9c2a504-faf8-4e10-f802-e4d13f7af984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80dfd1b9996a43658139c86e6aeeb83b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f78553923d384895a5fdd210153ee1bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 1000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import datasets\n",
        "import gc\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_x_train = datasets.Dataset.from_list(train_dataset).map(preprocess_function, batched=True)\n",
        "tokenized_x_dev = datasets.Dataset.from_list(dev_dataset).map(preprocess_function, batched=True)\n",
        "\n",
        "print(tokenized_x_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "   # Calculate F1-score\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "# The number of trainable layers of DistilledBERT\n",
        "num_layers = 104"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Distilled BERT"
      ],
      "metadata": {
        "id": "UBoUFniH_FDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5_7c_cHEwcJS",
        "outputId": "5883a92e-584d-4c03-8cbe-6be700493a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----Printing frozen layers 41-----\n",
            "distilbert.embeddings.word_embeddings.weight\n",
            "distilbert.embeddings.position_embeddings.weight\n",
            "distilbert.embeddings.LayerNorm.weight\n",
            "distilbert.embeddings.LayerNorm.bias\n",
            "distilbert.transformer.layer.0.attention.q_lin.weight\n",
            "distilbert.transformer.layer.0.attention.q_lin.bias\n",
            "distilbert.transformer.layer.0.attention.k_lin.weight\n",
            "distilbert.transformer.layer.0.attention.k_lin.bias\n",
            "distilbert.transformer.layer.0.attention.v_lin.weight\n",
            "distilbert.transformer.layer.0.attention.v_lin.bias\n",
            "distilbert.transformer.layer.0.attention.out_lin.weight\n",
            "distilbert.transformer.layer.0.attention.out_lin.bias\n",
            "distilbert.transformer.layer.0.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.0.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.0.ffn.lin1.weight\n",
            "distilbert.transformer.layer.0.ffn.lin1.bias\n",
            "distilbert.transformer.layer.0.ffn.lin2.weight\n",
            "distilbert.transformer.layer.0.ffn.lin2.bias\n",
            "distilbert.transformer.layer.0.output_layer_norm.weight\n",
            "distilbert.transformer.layer.0.output_layer_norm.bias\n",
            "distilbert.transformer.layer.1.attention.q_lin.weight\n",
            "distilbert.transformer.layer.1.attention.q_lin.bias\n",
            "distilbert.transformer.layer.1.attention.k_lin.weight\n",
            "distilbert.transformer.layer.1.attention.k_lin.bias\n",
            "distilbert.transformer.layer.1.attention.v_lin.weight\n",
            "distilbert.transformer.layer.1.attention.v_lin.bias\n",
            "distilbert.transformer.layer.1.attention.out_lin.weight\n",
            "distilbert.transformer.layer.1.attention.out_lin.bias\n",
            "distilbert.transformer.layer.1.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.1.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.1.ffn.lin1.weight\n",
            "distilbert.transformer.layer.1.ffn.lin1.bias\n",
            "distilbert.transformer.layer.1.ffn.lin2.weight\n",
            "distilbert.transformer.layer.1.ffn.lin2.bias\n",
            "distilbert.transformer.layer.1.output_layer_norm.weight\n",
            "distilbert.transformer.layer.1.output_layer_norm.bias\n",
            "distilbert.transformer.layer.2.attention.q_lin.weight\n",
            "distilbert.transformer.layer.2.attention.q_lin.bias\n",
            "distilbert.transformer.layer.2.attention.k_lin.weight\n",
            "distilbert.transformer.layer.2.attention.k_lin.bias\n",
            "distilbert.transformer.layer.2.attention.v_lin.weight\n",
            "\n",
            "Fine-tuning DistilledBERT frozen: 40 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 1:27:19, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.584700</td>\n",
              "      <td>0.290141</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.860848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.423200</td>\n",
              "      <td>0.282395</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.890699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.220700</td>\n",
              "      <td>0.234247</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.880343</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 01:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.23243600130081177, 'eval_accuracy': 0.89, 'eval_f1': 0.8901687979539642, 'eval_runtime': 104.8126, 'eval_samples_per_second': 0.954, 'eval_steps_per_second': 0.067, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----Printing frozen layers 62-----\n",
            "distilbert.embeddings.word_embeddings.weight\n",
            "distilbert.embeddings.position_embeddings.weight\n",
            "distilbert.embeddings.LayerNorm.weight\n",
            "distilbert.embeddings.LayerNorm.bias\n",
            "distilbert.transformer.layer.0.attention.q_lin.weight\n",
            "distilbert.transformer.layer.0.attention.q_lin.bias\n",
            "distilbert.transformer.layer.0.attention.k_lin.weight\n",
            "distilbert.transformer.layer.0.attention.k_lin.bias\n",
            "distilbert.transformer.layer.0.attention.v_lin.weight\n",
            "distilbert.transformer.layer.0.attention.v_lin.bias\n",
            "distilbert.transformer.layer.0.attention.out_lin.weight\n",
            "distilbert.transformer.layer.0.attention.out_lin.bias\n",
            "distilbert.transformer.layer.0.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.0.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.0.ffn.lin1.weight\n",
            "distilbert.transformer.layer.0.ffn.lin1.bias\n",
            "distilbert.transformer.layer.0.ffn.lin2.weight\n",
            "distilbert.transformer.layer.0.ffn.lin2.bias\n",
            "distilbert.transformer.layer.0.output_layer_norm.weight\n",
            "distilbert.transformer.layer.0.output_layer_norm.bias\n",
            "distilbert.transformer.layer.1.attention.q_lin.weight\n",
            "distilbert.transformer.layer.1.attention.q_lin.bias\n",
            "distilbert.transformer.layer.1.attention.k_lin.weight\n",
            "distilbert.transformer.layer.1.attention.k_lin.bias\n",
            "distilbert.transformer.layer.1.attention.v_lin.weight\n",
            "distilbert.transformer.layer.1.attention.v_lin.bias\n",
            "distilbert.transformer.layer.1.attention.out_lin.weight\n",
            "distilbert.transformer.layer.1.attention.out_lin.bias\n",
            "distilbert.transformer.layer.1.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.1.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.1.ffn.lin1.weight\n",
            "distilbert.transformer.layer.1.ffn.lin1.bias\n",
            "distilbert.transformer.layer.1.ffn.lin2.weight\n",
            "distilbert.transformer.layer.1.ffn.lin2.bias\n",
            "distilbert.transformer.layer.1.output_layer_norm.weight\n",
            "distilbert.transformer.layer.1.output_layer_norm.bias\n",
            "distilbert.transformer.layer.2.attention.q_lin.weight\n",
            "distilbert.transformer.layer.2.attention.q_lin.bias\n",
            "distilbert.transformer.layer.2.attention.k_lin.weight\n",
            "distilbert.transformer.layer.2.attention.k_lin.bias\n",
            "distilbert.transformer.layer.2.attention.v_lin.weight\n",
            "distilbert.transformer.layer.2.attention.v_lin.bias\n",
            "distilbert.transformer.layer.2.attention.out_lin.weight\n",
            "distilbert.transformer.layer.2.attention.out_lin.bias\n",
            "distilbert.transformer.layer.2.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.2.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.2.ffn.lin1.weight\n",
            "distilbert.transformer.layer.2.ffn.lin1.bias\n",
            "distilbert.transformer.layer.2.ffn.lin2.weight\n",
            "distilbert.transformer.layer.2.ffn.lin2.bias\n",
            "distilbert.transformer.layer.2.output_layer_norm.weight\n",
            "distilbert.transformer.layer.2.output_layer_norm.bias\n",
            "distilbert.transformer.layer.3.attention.q_lin.weight\n",
            "distilbert.transformer.layer.3.attention.q_lin.bias\n",
            "distilbert.transformer.layer.3.attention.k_lin.weight\n",
            "distilbert.transformer.layer.3.attention.k_lin.bias\n",
            "distilbert.transformer.layer.3.attention.v_lin.weight\n",
            "distilbert.transformer.layer.3.attention.v_lin.bias\n",
            "distilbert.transformer.layer.3.attention.out_lin.weight\n",
            "distilbert.transformer.layer.3.attention.out_lin.bias\n",
            "distilbert.transformer.layer.3.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.3.sa_layer_norm.bias\n",
            "\n",
            "Fine-tuning DistilledBERT frozen: 60 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 1:15:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.594100</td>\n",
              "      <td>0.400121</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.381600</td>\n",
              "      <td>0.238445</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.919333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.242000</td>\n",
              "      <td>0.216006</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.919333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 02:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.20826655626296997, 'eval_accuracy': 0.93, 'eval_f1': 0.92958605664488, 'eval_runtime': 165.1124, 'eval_samples_per_second': 0.606, 'eval_steps_per_second': 0.042, 'epoch': 2.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----Printing frozen layers 83-----\n",
            "distilbert.embeddings.word_embeddings.weight\n",
            "distilbert.embeddings.position_embeddings.weight\n",
            "distilbert.embeddings.LayerNorm.weight\n",
            "distilbert.embeddings.LayerNorm.bias\n",
            "distilbert.transformer.layer.0.attention.q_lin.weight\n",
            "distilbert.transformer.layer.0.attention.q_lin.bias\n",
            "distilbert.transformer.layer.0.attention.k_lin.weight\n",
            "distilbert.transformer.layer.0.attention.k_lin.bias\n",
            "distilbert.transformer.layer.0.attention.v_lin.weight\n",
            "distilbert.transformer.layer.0.attention.v_lin.bias\n",
            "distilbert.transformer.layer.0.attention.out_lin.weight\n",
            "distilbert.transformer.layer.0.attention.out_lin.bias\n",
            "distilbert.transformer.layer.0.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.0.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.0.ffn.lin1.weight\n",
            "distilbert.transformer.layer.0.ffn.lin1.bias\n",
            "distilbert.transformer.layer.0.ffn.lin2.weight\n",
            "distilbert.transformer.layer.0.ffn.lin2.bias\n",
            "distilbert.transformer.layer.0.output_layer_norm.weight\n",
            "distilbert.transformer.layer.0.output_layer_norm.bias\n",
            "distilbert.transformer.layer.1.attention.q_lin.weight\n",
            "distilbert.transformer.layer.1.attention.q_lin.bias\n",
            "distilbert.transformer.layer.1.attention.k_lin.weight\n",
            "distilbert.transformer.layer.1.attention.k_lin.bias\n",
            "distilbert.transformer.layer.1.attention.v_lin.weight\n",
            "distilbert.transformer.layer.1.attention.v_lin.bias\n",
            "distilbert.transformer.layer.1.attention.out_lin.weight\n",
            "distilbert.transformer.layer.1.attention.out_lin.bias\n",
            "distilbert.transformer.layer.1.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.1.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.1.ffn.lin1.weight\n",
            "distilbert.transformer.layer.1.ffn.lin1.bias\n",
            "distilbert.transformer.layer.1.ffn.lin2.weight\n",
            "distilbert.transformer.layer.1.ffn.lin2.bias\n",
            "distilbert.transformer.layer.1.output_layer_norm.weight\n",
            "distilbert.transformer.layer.1.output_layer_norm.bias\n",
            "distilbert.transformer.layer.2.attention.q_lin.weight\n",
            "distilbert.transformer.layer.2.attention.q_lin.bias\n",
            "distilbert.transformer.layer.2.attention.k_lin.weight\n",
            "distilbert.transformer.layer.2.attention.k_lin.bias\n",
            "distilbert.transformer.layer.2.attention.v_lin.weight\n",
            "distilbert.transformer.layer.2.attention.v_lin.bias\n",
            "distilbert.transformer.layer.2.attention.out_lin.weight\n",
            "distilbert.transformer.layer.2.attention.out_lin.bias\n",
            "distilbert.transformer.layer.2.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.2.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.2.ffn.lin1.weight\n",
            "distilbert.transformer.layer.2.ffn.lin1.bias\n",
            "distilbert.transformer.layer.2.ffn.lin2.weight\n",
            "distilbert.transformer.layer.2.ffn.lin2.bias\n",
            "distilbert.transformer.layer.2.output_layer_norm.weight\n",
            "distilbert.transformer.layer.2.output_layer_norm.bias\n",
            "distilbert.transformer.layer.3.attention.q_lin.weight\n",
            "distilbert.transformer.layer.3.attention.q_lin.bias\n",
            "distilbert.transformer.layer.3.attention.k_lin.weight\n",
            "distilbert.transformer.layer.3.attention.k_lin.bias\n",
            "distilbert.transformer.layer.3.attention.v_lin.weight\n",
            "distilbert.transformer.layer.3.attention.v_lin.bias\n",
            "distilbert.transformer.layer.3.attention.out_lin.weight\n",
            "distilbert.transformer.layer.3.attention.out_lin.bias\n",
            "distilbert.transformer.layer.3.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.3.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.3.ffn.lin1.weight\n",
            "distilbert.transformer.layer.3.ffn.lin1.bias\n",
            "distilbert.transformer.layer.3.ffn.lin2.weight\n",
            "distilbert.transformer.layer.3.ffn.lin2.bias\n",
            "distilbert.transformer.layer.3.output_layer_norm.weight\n",
            "distilbert.transformer.layer.3.output_layer_norm.bias\n",
            "distilbert.transformer.layer.4.attention.q_lin.weight\n",
            "distilbert.transformer.layer.4.attention.q_lin.bias\n",
            "distilbert.transformer.layer.4.attention.k_lin.weight\n",
            "distilbert.transformer.layer.4.attention.k_lin.bias\n",
            "distilbert.transformer.layer.4.attention.v_lin.weight\n",
            "distilbert.transformer.layer.4.attention.v_lin.bias\n",
            "distilbert.transformer.layer.4.attention.out_lin.weight\n",
            "distilbert.transformer.layer.4.attention.out_lin.bias\n",
            "distilbert.transformer.layer.4.sa_layer_norm.weight\n",
            "distilbert.transformer.layer.4.sa_layer_norm.bias\n",
            "distilbert.transformer.layer.4.ffn.lin1.weight\n",
            "distilbert.transformer.layer.4.ffn.lin1.bias\n",
            "distilbert.transformer.layer.4.ffn.lin2.weight\n",
            "distilbert.transformer.layer.4.ffn.lin2.bias\n",
            "distilbert.transformer.layer.4.output_layer_norm.weight\n",
            "\n",
            "Fine-tuning DistilledBERT frozen: 80 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 57:21, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.653900</td>\n",
              "      <td>0.577639</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.850586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.515200</td>\n",
              "      <td>0.345134</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.870516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.372600</td>\n",
              "      <td>0.285119</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.890436</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 01:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2843807339668274, 'eval_accuracy': 0.89, 'eval_f1': 0.8904363747329873, 'eval_runtime': 100.3676, 'eval_samples_per_second': 0.996, 'eval_steps_per_second': 0.07, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter on how many layers to freeze [40%, 60%, 80%]\n",
        "\n",
        "layers_to_freeze = [int(num_layers * 0.4), int(num_layers * 0.6), int(num_layers * 0.8)]\n",
        "metrics = dict()\n",
        "\n",
        "\n",
        "for portion in layers_to_freeze:\n",
        "\n",
        "  proxy_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "  print('\\n-----Printing frozen layers {}-----'.format(portion))\n",
        "\n",
        "  for name, param in list(proxy_model.named_parameters())[:portion]:\n",
        "    if param.requires_grad == True:\n",
        "      print(name)\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./txt_cls_example{}/'.format(portion),\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=20,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "  )\n",
        "\n",
        "  print('\\nFine-tuning DistilledBERT frozen:', (int((portion / num_layers) * 100) + 1), '%')\n",
        "  trainer = Trainer(\n",
        "      proxy_model,\n",
        "      training_args,\n",
        "      train_dataset=tokenized_x_train,\n",
        "      eval_dataset=tokenized_x_dev,\n",
        "      data_collator=data_collator,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  metrics[portion] = trainer.evaluate()\n",
        "\n",
        "  print(metrics[portion])\n",
        "\n",
        "  del proxy_model\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SELCMRCTw9gN"
      },
      "source": [
        "# Evaluation on Train, Development and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZfuyNMVqIo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "09892ba5-c647-40bb-9003-e2e00e37f52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fine-tuning DistilledBERT frozen: 60 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 1:22:40, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.614500</td>\n",
              "      <td>0.340392</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.870717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.403400</td>\n",
              "      <td>0.226958</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.909011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.252000</td>\n",
              "      <td>0.206080</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.918890</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 01:36]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1954440474510193, 'eval_accuracy': 0.92, 'eval_f1': 0.918890290037831, 'eval_runtime': 115.4738, 'eval_samples_per_second': 0.866, 'eval_steps_per_second': 0.061, 'epoch': 2.0}\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "for name, param in list(model.named_parameters())[:int(num_layers * 0.6)]:\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./txt_cls_exampleBest/',\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=20,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "  )\n",
        "\n",
        "print('\\nFine-tuning DistilledBERT frozen: 60 %')\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "      model,\n",
        "      training_args,\n",
        "      train_dataset=tokenized_x_train,\n",
        "      eval_dataset=tokenized_x_dev,\n",
        "      data_collator=data_collator,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(trainer.evaluate())\n",
        "\n",
        "\n",
        "# We use a random sample from train, test and development datasets for evaluation\n",
        "\n",
        "train_set = [' '.join(x_doc) for x_doc in x_train_tokenized[:100]]\n",
        "dev_set = [' '.join(x_doc) for x_doc in x_dev_tokenized[:100]]\n",
        "test_set = [' '.join(x_doc) for x_doc in x_test_tokenized[400:500]]\n",
        "\n",
        "\n",
        "tokenized_x_train = tokenizer(train_set, truncation=True, padding=True, return_tensors='pt')\n",
        "tokenized_x_dev = tokenizer(dev_set, truncation=True, padding=True, return_tensors='pt')\n",
        "tokenized_x_test = tokenizer(test_set, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  predictions_train = model(**tokenized_x_train)\n",
        "  predictions_dev = model(**tokenized_x_dev)\n",
        "  predictions_test = model(**tokenized_x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        " return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "y_predictions_train = [np.argmax(softmax(x)) for x in np.array(predictions_train.logits)]\n",
        "y_predictions_dev = [np.argmax(softmax(x)) for x in np.array(predictions_dev.logits)]\n",
        "y_predictions_test = [np.argmax(softmax(x)) for x in np.array(predictions_test.logits)]\n",
        "\n",
        "\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "c6zlwjDFaZTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlfhhqH_e6pa"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Precision , Recall , F1 , AUC scores for Distilled BERT classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vykqg62ZMWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b9cb5be-5774-4c04-c124-7bf8cf08759e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------- Fine-tuned Distilled BERT ---------\n",
            "\n",
            "Training set\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.92      0.92      0.92        59\n",
            "    positive       0.88      0.88      0.88        41\n",
            "\n",
            "    accuracy                           0.90       100\n",
            "   macro avg       0.90      0.90      0.90       100\n",
            "weighted avg       0.90      0.90      0.90       100\n",
            "\n",
            "AUC training : 0.9030487804878049 \n",
            "\n",
            "\n",
            "Development set\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      0.91      0.92        46\n",
            "    positive       0.93      0.94      0.94        54\n",
            "\n",
            "    accuracy                           0.93       100\n",
            "   macro avg       0.93      0.93      0.93       100\n",
            "weighted avg       0.93      0.93      0.93       100\n",
            "\n",
            "AUC development : 0.9508585858585858 \n",
            "\n",
            "\n",
            "Test set\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.92      0.84      0.88        55\n",
            "    positive       0.82      0.91      0.86        45\n",
            "\n",
            "    accuracy                           0.87       100\n",
            "   macro avg       0.87      0.87      0.87       100\n",
            "weighted avg       0.88      0.87      0.87       100\n",
            "\n",
            "AUC test : 0.8855555555555554\n"
          ]
        }
      ],
      "source": [
        "distillbert_precision_train, distillbert_recall_train, thresholds = precision_recall_curve(y_train[:100], y_predictions_train)\n",
        "\n",
        "distillbert_precision_dev, distillbert_recall_dev, thresholds = precision_recall_curve(y_dev[:100], y_predictions_dev)\n",
        "\n",
        "distillbert_precision_test, distillbert_recall_test, thresholds = precision_recall_curve(y_test[400:500], y_predictions_test)\n",
        "\n",
        "\n",
        "print('\\n--------- Fine-tuned Distilled BERT ---------\\n')\n",
        "print('Training set\\n')\n",
        "print(classification_report(y_train[:100], y_predictions_train, target_names = target_names))\n",
        "print('AUC training :', auc(distillbert_recall_train, distillbert_precision_train), '\\n')\n",
        "print('\\nDevelopment set\\n')\n",
        "print(classification_report(y_dev[:100], y_predictions_dev, target_names = target_names))\n",
        "print('AUC development :', auc(distillbert_recall_dev, distillbert_precision_dev), '\\n')\n",
        "print('\\nTest set\\n')\n",
        "print(classification_report(y_test[400:500], y_predictions_test, target_names = target_names))\n",
        "print('AUC test :', auc(distillbert_recall_test, distillbert_precision_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80dfd1b9996a43658139c86e6aeeb83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0953473037b741479fbc85ca00942ef8",
              "IPY_MODEL_d859419f0b2d4d0ca004473aeeac8694",
              "IPY_MODEL_153c329214c54c85be6470c8bfa2b410"
            ],
            "layout": "IPY_MODEL_034bbc8e52c54206988ac58209492cde"
          }
        },
        "0953473037b741479fbc85ca00942ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a12e889ab944419e85f2e967337061",
            "placeholder": "​",
            "style": "IPY_MODEL_0f1186ee76ef4191b5b0d98febaedfb5",
            "value": "Map: 100%"
          }
        },
        "d859419f0b2d4d0ca004473aeeac8694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_790a5ce5bb3945699673ce2df5ce1c34",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7633a074e1a34615b84e2ed749b5cc55",
            "value": 1000
          }
        },
        "153c329214c54c85be6470c8bfa2b410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7810bc4ddb0d425ea75ab36f4b968e98",
            "placeholder": "​",
            "style": "IPY_MODEL_c4cd86cb33e843b08ec552c864a9cd8e",
            "value": " 1000/1000 [00:03&lt;00:00, 296.40 examples/s]"
          }
        },
        "034bbc8e52c54206988ac58209492cde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a12e889ab944419e85f2e967337061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f1186ee76ef4191b5b0d98febaedfb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "790a5ce5bb3945699673ce2df5ce1c34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7633a074e1a34615b84e2ed749b5cc55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7810bc4ddb0d425ea75ab36f4b968e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4cd86cb33e843b08ec552c864a9cd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f78553923d384895a5fdd210153ee1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2552bfc23db49cea84d421d85ff357c",
              "IPY_MODEL_004ec604b7f943ee9c89f5e4f4aafb78",
              "IPY_MODEL_50f40a9bc5df4d638c5f82a2bfef11ed"
            ],
            "layout": "IPY_MODEL_84376e23d7934bab8f904e5cd1656751"
          }
        },
        "a2552bfc23db49cea84d421d85ff357c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4bf80931cf84221ba79a78a5613d914",
            "placeholder": "​",
            "style": "IPY_MODEL_0edbe93569784b42af374bb6f56e4ea0",
            "value": "Map: 100%"
          }
        },
        "004ec604b7f943ee9c89f5e4f4aafb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac38470dd42148a086e7435fd12fdd8b",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a03cf31fb0c4334b63146398c37031b",
            "value": 100
          }
        },
        "50f40a9bc5df4d638c5f82a2bfef11ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbb13cab0cef45c3874484a0f3e37a35",
            "placeholder": "​",
            "style": "IPY_MODEL_c7ca243a068643bdb6aefe386751f771",
            "value": " 100/100 [00:00&lt;00:00, 473.42 examples/s]"
          }
        },
        "84376e23d7934bab8f904e5cd1656751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4bf80931cf84221ba79a78a5613d914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0edbe93569784b42af374bb6f56e4ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac38470dd42148a086e7435fd12fdd8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a03cf31fb0c4334b63146398c37031b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbb13cab0cef45c3874484a0f3e37a35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7ca243a068643bdb6aefe386751f771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}